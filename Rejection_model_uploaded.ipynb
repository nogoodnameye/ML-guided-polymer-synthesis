{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "import category_encoders as ce\n",
    "from hyperopt import hp, tpe, Trials, STATUS_OK, fmin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from rdkit import DataStructs, Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import shap\n",
    "from sklearn.model_selection import cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor,AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,GradientBoostingRegressor \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge,ARDRegression,BayesianRidge,ElasticNet,GammaRegressor,HuberRegressor\n",
    "from sklearn.linear_model import Lasso, LassoLars, LinearRegression, LogisticRegression, PassiveAggressiveRegressor,Ridge,SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data = pd.read_csv('Rej_0416.csv')\n",
    "hh = data.drop(columns=['salt rejection','Monomer A1 type','Monomer A2 type'])\n",
    "data.drop(index = hh[hh.duplicated(keep='first')==True].index, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class morgan_fp:\n",
    "    def __init__(self, radius, length):\n",
    "        self.radius = radius\n",
    "        self.length = length\n",
    "    def __call__(self, smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, self.radius, self.length)\n",
    "        npfp = np.array(list(fp.ToBitString())).astype('float32')\n",
    "        return npfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = data.select_dtypes(include=['object']).drop(['Monomer A1 type','a-smile', 'Monomer A2 type', 'b-smile'], axis=1).columns\n",
    "numeric_features = data.select_dtypes(include=['int64', 'float64']).drop(['salt rejection'], axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=50)\n",
    "sss.split(data)\n",
    "for train_index, test_index in sss.split(data):\n",
    "    train_data = data.iloc[train_index]\n",
    "    test = data.iloc[test_index]\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    train_data.to_csv('salt_train_data.csv', index = False)\n",
    "    test.to_csv('salt_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_data_pd(data, data_t, fp, en, scaler):\n",
    "    data['a-fp'] = data['a-smile'].apply(fp)\n",
    "    data['b-fp'] = data['b-smile'].apply(fp)\n",
    "    x_a=np.array(list(data['a-fp']))\n",
    "    x_b=np.array(list(data['b-fp']))\n",
    "    for i in range(len(data)):\n",
    "        if data['b-smile'][i]=='C':\n",
    "            x_b[i]=x_b[i]*0\n",
    "    \n",
    "    X_train = data_t.drop(columns=['Monomer A1 type', 'a-smile','Monomer A2 type', 'b-smile', 'salt rejection', 'a-fp', 'b-fp']).copy()\n",
    "    Y_train = data_t['salt rejection'].copy()\n",
    "    hh=en.fit_transform(X_train, Y_train)\n",
    "    SC= scaler.fit(hh[numeric_features])\n",
    "    \n",
    "    x = data.drop(columns=['Monomer A1 type', 'a-smile','Monomer A2 type', 'b-smile', 'salt rejection', 'a-fp', 'b-fp']).copy()\n",
    "    y = data['salt rejection'].copy()\n",
    "    xx = en.transform(x, y)\n",
    "    col_pd = xx.drop(columns= numeric_features) #pd\n",
    "    xxxx = SC.transform(xx[numeric_features])\n",
    "    num_pd = pd.DataFrame(data= xxxx, columns=numeric_features) #pd\n",
    "    \n",
    "    xxxxx = np.concatenate((x_a, x_b), axis =1)\n",
    "    fp_pd = pd.DataFrame(data= xxxxx, columns=[f'f_{i}' for i in range(2*x_a.shape[1])])\n",
    "    \n",
    "    x_pd = pd.concat([fp_pd, col_pd, num_pd], axis =1)\n",
    "    y = data['salt rejection'].values\n",
    "    \n",
    "    return x_pd, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = morgan_fp(1,2048)\n",
    "train_data = pd.read_csv('salt_train_data.csv')\n",
    "test_data = pd.read_csv('salt_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(shuffle=True, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [CatBoostRegressor(verbose=False, random_state=10), XGBRegressor(random_state=10)]\n",
    "encoder = [ce.backward_difference.BackwardDifferenceEncoder(cols = categorical_features), \n",
    "               ce.basen.BaseNEncoder(cols = categorical_features),\n",
    "               ce.binary.BinaryEncoder(cols = categorical_features), \n",
    "                ce.helmert.HelmertEncoder(cols = categorical_features),\n",
    "                ce.james_stein.JamesSteinEncoder(cols = categorical_features),\n",
    "                ce.one_hot.OneHotEncoder(cols = categorical_features),\n",
    "                ce.m_estimate.MEstimateEncoder(cols = categorical_features),\n",
    "                ce.sum_coding.SumEncoder(cols = categorical_features)]\n",
    "scaler = [StandardScaler(), MinMaxScaler(), MaxAbsScaler(), RobustScaler(),PowerTransformer()]\n",
    "results = pd.DataFrame(columns=['train_rmse','train_r2', 'test_rmse','test_r2', 'name', 'encoder', 'scaler'])\n",
    "\n",
    "k =0 \n",
    "for model in models:\n",
    "    for sc in scaler:\n",
    "        for en in encoder:\n",
    "            t_rmse=[]\n",
    "            t_r2=[]\n",
    "            v_rmse=[]\n",
    "            v_r2=[]\n",
    "            for train_index, test_index in kf.split(train_data):\n",
    "                train = train_data.loc[train_index]\n",
    "                train.reset_index(drop=True, inplace=True)\n",
    "                val = train_data.loc[test_index]\n",
    "                val.reset_index(drop=True, inplace=True)\n",
    "                x_train_pd, y_train = conv_data_pd(train, train, fp, en, sc)\n",
    "                x_train = x_train_pd.values\n",
    "                x_val_pd, y_val = conv_data_pd(val, train, fp, en,sc)\n",
    "                x_val = x_val_pd.values\n",
    "                model.fit(x_train, y_train)\n",
    "                y_val_pred = model.predict(x_val)\n",
    "                y_train_pred = model.predict(x_train)\n",
    "                t_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "                v_rmse.append(np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
    "                t_r2.append(r2_score(y_train,y_train_pred))\n",
    "                v_r2.append(r2_score(y_val, y_val_pred))\n",
    "            results.loc[k, 'train_rmse']=np.mean(t_rmse)\n",
    "            results.loc[k, 'test_rmse']=np.mean(v_rmse)\n",
    "            results.loc[k, 'train_r2']=np.mean(t_r2)\n",
    "            results.loc[k, 'test_r2']=np.mean(v_r2)\n",
    "            results.loc[k, 'name']=model.__class__.__name__\n",
    "            results.loc[k, 'encoder']=en.__class__.__name__\n",
    "            results.loc[k, 'scaler']=sc.__class__.__name__\n",
    "            k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['test_r2'], ascending= False, inplace = True)\n",
    "results.to_excel('salt_model_selection.xlsx', index =False)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {    \n",
    "    'max_delta_step': hp.uniform('max_delta_step', 0, 10),\n",
    "         'learning_rate': hp.uniform('learning_rate', 0, 1),\n",
    "        'max_depth': hp.quniform('max_depth', 1,6,1),\n",
    "         'min_child_weight': hp.uniform('min_child_weight', 1,100),\n",
    "         'subsample': hp.uniform('subsample', 0.0, 1.0),\n",
    "         'reg_alpha':hp.uniform('reg_alpha', 0, 100),\n",
    "         'gamma':hp.uniform('gamma', 0, 100),\n",
    "         'reg_lambda':hp.uniform('reg_lambda', 0, 100),\n",
    "    'n_estimators': hp.quniform('n_estimators', 1, 100, 1),\n",
    "        'fp_length': hp.quniform('fp_length', 10, 5048, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "sc = RobustScaler()\n",
    "en = ce.helmert.HelmertEncoder(cols = categorical_features)\n",
    "def fit(params):\n",
    "    fp = morgan_fp(0, params['fp_length'])\n",
    "    model = XGBRegressor(max_delta_step=params['max_delta_step'], learning_rate = params['learning_rate'],\n",
    "                    max_depth = params['max_depth'], min_child_weight= params['min_child_weight'],\n",
    "                    subsample=params['subsample'],reg_alpha=params['reg_alpha'],gamma= params['gamma'],\n",
    "                    reg_lambda= params['reg_lambda'],n_estimators=params['n_estimators'], random_state=10)\n",
    "    t_rmse=[]\n",
    "    v_rmse=[]\n",
    "    for train_index, test_index in kf.split(train_data):\n",
    "        train = train_data.loc[train_index]\n",
    "        train.reset_index(drop=True, inplace=True)\n",
    "        val = train_data.loc[test_index]\n",
    "        val.reset_index(drop=True, inplace=True)\n",
    "        x_train_pd, y_train = conv_data_pd(train, train, fp, en, sc)\n",
    "        x_train = x_train_pd.values\n",
    "        x_val_pd, y_val = conv_data_pd(val, train, fp, en, sc)\n",
    "        x_val = x_val_pd.values\n",
    "        model.fit(x_train, y_train)\n",
    "        t_rmse.append(np.sqrt(mean_squared_error(y_train, model.predict(x_train))))\n",
    "        v_rmse.append(np.sqrt(mean_squared_error(y_val, model.predict(x_val))))\n",
    "    \n",
    "    return np.mean(v_rmse), np.mean(t_rmse)\n",
    "\n",
    "def objective(params):\n",
    "    global ITERATION\n",
    "    ITERATION +=1\n",
    "    for name in ['max_depth', 'n_estimators', 'fp_radius', 'fp_length']:\n",
    "        params[name] = int(params[name])\n",
    "    loss, train_loss = fit(params)\n",
    "    loss =loss\n",
    "    off_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(off_connection)\n",
    "    writer.writerow([loss,train_loss, params, ITERATION])\n",
    "    pickle.dump(bayes_trial, open(\"rej_1.p\", \"wb\"))\n",
    "    return {'loss':loss,'train_loss':train_loss, 'params': params, 'iteration':ITERATION, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "out_file ='rej.csv'\n",
    "off_connection =open(out_file, 'w')\n",
    "writer = csv.writer(off_connection)\n",
    "writer.writerow(['loss','train_loss', 'params', 'iteration'])\n",
    "off_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpe_algo = tpe.suggest\n",
    "bayes_trial = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%%capture\n",
    "global ITERATION\n",
    "ITERATION =0\n",
    "best = fmin(fn = objective, space =space, algo = tpe_algo, trials = bayes_trial, max_evals=3000, rstate= np.random.RandomState(50)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('rej.csv')\n",
    "result.sort_values('loss', ascending= True, inplace = True)\n",
    "result.reset_index(drop = True, inplace =True)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "params = ast.literal_eval(result['params'][0])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = morgan_fp(0, params['fp_length'])\n",
    "#fp = morgan_fp(0, 3000)\n",
    "sc = RobustScaler()\n",
    "en = ce.helmert.HelmertEncoder(cols = categorical_features)\n",
    "x_train_pd, y_train = conv_data_pd(train_data, train_data, fp, en, sc)\n",
    "x_train=x_train_pd.values\n",
    "model = XGBRegressor(max_delta_step=params['max_delta_step'], learning_rate = params['learning_rate'],\n",
    "                    max_depth = params['max_depth'], min_child_weight= params['min_child_weight'],\n",
    "                    subsample=params['subsample'],reg_alpha=params['reg_alpha'],gamma= params['gamma'],\n",
    "                    reg_lambda= params['reg_lambda'],n_estimators=params['n_estimators'])\n",
    "model.save_model('rej_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train)\n",
    "y_train_pred = model.predict(x_train)\n",
    "x_test_pd, y_test = conv_data_pd(test_data, train_data, fp, en, sc)\n",
    "x_test=x_test_pd.values\n",
    "y_test_pred = model.predict(x_test)\n",
    "r2_score(y_test, y_test_pred), r2_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_test, y_test_pred)), np.sqrt(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train, y_train_pred, label='Train')\n",
    "plt.scatter(y_test, y_test_pred, label='Test')\n",
    "plt.xlim(0,110)\n",
    "plt.ylim(0,110)\n",
    "xx=range(0,110)\n",
    "yy=xx\n",
    "plt.plot(xx, yy, '--', linewidth=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['pred']=y_train_pred\n",
    "test_data['pred']=y_test_pred\n",
    "train_data.to_excel('salt_train_pred.xlsx', index=False)\n",
    "test_data.to_excel('salt_test_pred.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
